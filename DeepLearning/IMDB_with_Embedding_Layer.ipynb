{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNvtgZWM2v3IaLvk2JbT9Bx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"Scy-c5YrvKYR"},"source":["# 단어 수준의 원-핫 인코딩하기\n","import numpy as np\n","samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n","\n","token_index = {} # 데이터에 있는 모든 토큰의 인덱스를 구축합니다.\n","for sample in samples:\n","  for word in sample.split(): # split() 메서드를 사용하여 샘플을 토큰으로 나눕니다. 실전에서는 구두점과 특수 문자도 사용합니다.\n","    if word not in token_index:\n","      token_index[word] = len(token_index) + 1\n","\n","max_length = 10 # 샘플을 벡터로 변환합니다. 각 샘플에서 max_length까지 단어만 사용합니다.\n","\n","results = np.zeros(shape=(len(samples),\n","                   max_length,\n","                   max(token_index.values()) + 1))\n","for i, sample in enumerate(samples):\n","  for j, word in list(enumerate(sample.split()))[:max_length]:\n","    index = token_index.get(word)\n","    results[i, j, index] = 1."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bSD9oQWWvd0x","executionInfo":{"status":"ok","timestamp":1631263881419,"user_tz":-540,"elapsed":311,"user":{"displayName":"이찬우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggtc8XiGVf5dkYUXiOAPSVmIanx0t_Rp-frSbwF=s64","userId":"15145242207001469329"}},"outputId":"3855ec73-58b8-4401-99f7-c7eb11011d6e"},"source":["import string\n","samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n","characters = string.printable # 출력 가능한 모든 아스키 문자\n","token_index = dict(zip(characters, range(1, len(characters) + 1)))\n","\n","max_length = 50\n","results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n","for i, sample in enumerate(samples):\n","  for j, character in enumerate(sample):\n","    index = token_index.get(character)\n","    results[i, j, index] = 1.\n","print(results)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[[0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]]\n","\n"," [[0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]]]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"egJyOfFU3QYi","executionInfo":{"status":"ok","timestamp":1631263457479,"user_tz":-540,"elapsed":300,"user":{"displayName":"이찬우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggtc8XiGVf5dkYUXiOAPSVmIanx0t_Rp-frSbwF=s64","userId":"15145242207001469329"}},"outputId":"318fc760-ada2-4909-ddf1-af6d7cd58015"},"source":["# 케라스를 사용한 단어 수준의 원-핫 인코딩하기\n","from keras.preprocessing.text import Tokenizer\n","samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n","tokenizer = Tokenizer(num_words = 1000) # num_words=1000: 가장 빈도수가 높은 단어만 선택하여 tokenizer객체에 저장\n","tokenizer.fit_on_texts(samples) # 단어 인덱스를 구축합니다.\n","\n","sequences = tokenizer.texts_to_sequences(samples)\n","\n","one_hot_results = tokenizer.texts_to_matrix(samples, mode = 'binary')\n","\n","word_index = tokenizer.word_index\n","print('%s 개의 고유한 토큰을 찾았습니다.' % len(word_index))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["9 개의 고유한 토큰을 찾았습니다.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_35FFgyt4P3I","executionInfo":{"status":"ok","timestamp":1631263648640,"user_tz":-540,"elapsed":327,"user":{"displayName":"이찬우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggtc8XiGVf5dkYUXiOAPSVmIanx0t_Rp-frSbwF=s64","userId":"15145242207001469329"}},"outputId":"4c3e30eb-3741-408a-ccf8-bec359a90abf"},"source":["print(tokenizer.word_index)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}\n"]}]},{"cell_type":"code","metadata":{"id":"ZhoBAICN4-hf"},"source":["# 해싱 기법을 사용한 단어 수준의 원-핫 인코딩하기\n","samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n","\n","dimensionality = 1000 # 단어를 크기가 1000인 벡터로 저장합니다. 1000개의 단어가 있다면 해싱 충돌이 늘어나고, 인코딩의 정확도가 감소합니다.\n","max_length = 10\n","\n","results = np.zeros((len(samples), max_length, dimensionality))\n","for i, sample in enumerate(samples):\n","  for j, word in list(enumerate(sample.split()))[:max_length]:\n","    index = abs(hash(word)) % dimensionality\n","    results[i, j, index] = 1."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vZmjZdym5ycf","executionInfo":{"status":"ok","timestamp":1631263868113,"user_tz":-540,"elapsed":301,"user":{"displayName":"이찬우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggtc8XiGVf5dkYUXiOAPSVmIanx0t_Rp-frSbwF=s64","userId":"15145242207001469329"}},"outputId":"450ff365-dbb4-4d1c-d3ca-27ac06a7b250"},"source":["print(results)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[[0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]]\n","\n"," [[0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]]]\n"]}]},{"cell_type":"code","metadata":{"id":"Ikf6z5Tf50HO"},"source":["''' Embeddong 층의 객체 생성하기 '''\n","from keras.layers import Embedding\n","\n","embedding_layer = Embedding(1000, 64)\n","# Embedding 층은 적어도 2개의 매개변수를 받습니다.\n","# 가능한 토큰의 개수(여기서는 10000으로 단어 인덱스 최대값 + 1입니다.)와 임베딩 차원(여기서는 64) 입니다."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3UMB05S67adw"},"source":["# Embedding 층에 사용할 IMDB 데이터 로드하기\n","from keras.datasets import imdb\n","from keras import preprocessing\n","\n","max_features = 10000\n","maxlen = 10\n","\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = max_features)\n","\n","# 리스트를 (samples, maxlen) 크기의 2D 정수 텐서로 변환합니다.\n","x_train = preprocessing.sequence.pad_sequences(x_train, maxlen = maxlen)\n","x_test = preprocessing.sequence.pad_sequences(x_test, maxlen = maxlen)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MMyXKL4R864o","executionInfo":{"status":"ok","timestamp":1631264937727,"user_tz":-540,"elapsed":11084,"user":{"displayName":"이찬우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggtc8XiGVf5dkYUXiOAPSVmIanx0t_Rp-frSbwF=s64","userId":"15145242207001469329"}},"outputId":"cf1f4e04-1709-451d-8180-8faf1ea238fe"},"source":["# IMDB 데이터에 Embedding 층과 분류기 사용하기\n","from keras.models import Sequential\n","from keras.layers import Flatten, Dense, Embedding\n","\n","model = Sequential()\n","model.add(Embedding(10000, 8, input_length = maxlen))\n","\n","model.add(Flatten())\n","\n","model.add(Dense(1, activation = 'sigmoid'))\n","model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['acc'])\n","model.summary()\n","\n","history = model.fit(x_train, y_train,\n","                    epochs = 10,\n","                    batch_size = 32,\n","                    validation_split = 0.2)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_2 (Embedding)      (None, 10, 8)             80000     \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 80)                0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 81        \n","=================================================================\n","Total params: 80,081\n","Trainable params: 80,081\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/10\n","625/625 [==============================] - 2s 2ms/step - loss: 0.6766 - acc: 0.5975 - val_loss: 0.6453 - val_acc: 0.6602\n","Epoch 2/10\n","625/625 [==============================] - 1s 2ms/step - loss: 0.5897 - acc: 0.7132 - val_loss: 0.5724 - val_acc: 0.6974\n","Epoch 3/10\n","625/625 [==============================] - 1s 2ms/step - loss: 0.5193 - acc: 0.7488 - val_loss: 0.5454 - val_acc: 0.7126\n","Epoch 4/10\n","625/625 [==============================] - 1s 2ms/step - loss: 0.4820 - acc: 0.7696 - val_loss: 0.5389 - val_acc: 0.7160\n","Epoch 5/10\n","625/625 [==============================] - 1s 2ms/step - loss: 0.4579 - acc: 0.7857 - val_loss: 0.5379 - val_acc: 0.7212\n","Epoch 6/10\n","625/625 [==============================] - 1s 2ms/step - loss: 0.4381 - acc: 0.8007 - val_loss: 0.5421 - val_acc: 0.7188\n","Epoch 7/10\n","625/625 [==============================] - 1s 2ms/step - loss: 0.4212 - acc: 0.8107 - val_loss: 0.5479 - val_acc: 0.7192\n","Epoch 8/10\n","625/625 [==============================] - 1s 2ms/step - loss: 0.4054 - acc: 0.8196 - val_loss: 0.5516 - val_acc: 0.7202\n","Epoch 9/10\n","625/625 [==============================] - 1s 2ms/step - loss: 0.3914 - acc: 0.8288 - val_loss: 0.5588 - val_acc: 0.7192\n","Epoch 10/10\n","625/625 [==============================] - 1s 2ms/step - loss: 0.3777 - acc: 0.8358 - val_loss: 0.5654 - val_acc: 0.7170\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3l3mgLmT9zfF","executionInfo":{"status":"ok","timestamp":1631266044595,"user_tz":-540,"elapsed":1239,"user":{"displayName":"이찬우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggtc8XiGVf5dkYUXiOAPSVmIanx0t_Rp-frSbwF=s64","userId":"15145242207001469329"}},"outputId":"d4fbcddf-a891-4e3f-97c9-de118648c98a"},"source":["model.evaluate(x_test, y_test)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["782/782 [==============================] - 1s 969us/step - loss: 0.5733 - acc: 0.7149\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.5732532143592834, 0.7148799896240234]"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"KokotPa1CEXu"},"source":[],"execution_count":null,"outputs":[]}]}